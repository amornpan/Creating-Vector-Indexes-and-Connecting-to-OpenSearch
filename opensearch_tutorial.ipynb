{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# Building Vector Indexes and OpenSearch Integration\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this tutorial, we'll learn how to create a **Hybrid Search** system that combines semantic search and keyword search using:\n",
        "- **OpenSearch** for vector database and keyword search\n",
        "- **LlamaIndex** for document management and indexing\n",
        "- **BGE-M3** embedding model for text-to-vector conversion\n",
        "\n",
        "### What is Hybrid Search?\n",
        "Hybrid search combines:\n",
        "- **Keyword Search**: Exact term matching (good for precise queries)\n",
        "- **Semantic Search**: Understanding context and meaning (good for conceptual queries)\n",
        "- **Combined Results**: More comprehensive and accurate results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section1_header"
      },
      "source": [
        "## Section 1: Installing Dependencies\n",
        "\n",
        "First, we need to install all required packages for our hybrid search system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install LlamaIndex and dependencies\n",
        "!pip install llama-index -q\n",
        "!pip install llama-index-embeddings-huggingface -q\n",
        "!pip install llama-index-vector-stores-opensearch -q\n",
        "!pip install requests -q\n",
        "!pip install nest_asyncio -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dependencies_explanation"
      },
      "source": [
        "**Package Explanations:**\n",
        "- `llama-index`: Main framework for building RAG applications\n",
        "- `llama-index-embeddings-huggingface`: For using Hugging Face embedding models\n",
        "- `llama-index-vector-stores-opensearch`: OpenSearch connector\n",
        "- `requests`: For HTTP API calls\n",
        "- `nest_asyncio`: Fixes event loop issues in Jupyter Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section2_header"
      },
      "source": [
        "## Section 2: Import Modules and Initial Setup\n",
        "\n",
        "Import all necessary modules and configure the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_modules"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import urllib.request\n",
        "import pickle\n",
        "import requests\n",
        "import nest_asyncio\n",
        "import json\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.opensearch import OpensearchVectorStore, OpensearchVectorClient\n",
        "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
        "\n",
        "# Apply nest_asyncio to avoid runtime errors\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"‚úÖ All modules imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports_explanation"
      },
      "source": [
        "**Key Imports:**\n",
        "- **Core LlamaIndex**: Document reading, indexing, and storage\n",
        "- **Node Parser**: For splitting documents into manageable chunks\n",
        "- **Embedding**: For converting text to vectors\n",
        "- **Vector Store**: For OpenSearch integration\n",
        "- **nest_asyncio**: Fixes async issues in Jupyter environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section3_header"
      },
      "source": [
        "## Section 3: Configuration Setup\n",
        "\n",
        "‚ö†Ô∏è **Important**: Change `OPENSEARCH_INDEX` to your unique name (e.g., `yourname_doc_index`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "configuration"
      },
      "outputs": [],
      "source": [
        "# OpenSearch Configuration\n",
        "OPENSEARCH_ENDPOINT = \"http://34.101.178.186:9200\"\n",
        "OPENSEARCH_INDEX = \"yourname_doc_index\"  # ‚ö†Ô∏è CHANGE THIS TO YOUR NAME\n",
        "TEXT_FIELD = \"content\"\n",
        "EMBEDDING_FIELD = \"embedding\"\n",
        "\n",
        "# Check if CUDA is available for GPU acceleration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üñ•Ô∏è Using device: {device}\")\n",
        "\n",
        "# Display configuration\n",
        "print(f\"üîó OpenSearch Endpoint: {OPENSEARCH_ENDPOINT}\")\n",
        "print(f\"üìä Index Name: {OPENSEARCH_INDEX}\")\n",
        "print(f\"üìù Text Field: {TEXT_FIELD}\")\n",
        "print(f\"üî¢ Embedding Field: {EMBEDDING_FIELD}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_explanation"
      },
      "source": [
        "**Configuration Details:**\n",
        "- **Endpoint**: OpenSearch cluster URL\n",
        "- **Index Name**: Must be unique for each user (lowercase)\n",
        "- **Fields**: Specify where text content and embeddings are stored\n",
        "- **Device**: GPU acceleration if available, otherwise CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section4_header"
      },
      "source": [
        "## Section 4: Creating Hybrid Search Pipeline\n",
        "\n",
        "This creates a search pipeline that combines keyword and semantic search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hybrid_pipeline"
      },
      "outputs": [],
      "source": [
        "def create_hybrid_search_pipeline():\n",
        "    \"\"\"Create a hybrid search pipeline in OpenSearch\"\"\"\n",
        "    pipeline_url = f\"{OPENSEARCH_ENDPOINT}/_search/pipeline/hybrid-search-pipeline\"\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    pipeline_config = {\n",
        "        \"description\": \"Pipeline for hybrid search\",\n",
        "        \"phase_results_processors\": [\n",
        "            {\n",
        "                \"normalization-processor\": {\n",
        "                    \"normalization\": {\n",
        "                        \"technique\": \"min_max\"  # Normalize scores to 0-1 range\n",
        "                    },\n",
        "                    \"combination\": {\n",
        "                        \"technique\": \"harmonic_mean\",\n",
        "                        \"parameters\": {\n",
        "                            \"weights\": [0.3, 0.7]  # keyword: 30%, semantic: 70%\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.put(pipeline_url, headers=headers, data=json.dumps(pipeline_config))\n",
        "        if response.status_code in [200, 201]:\n",
        "            print(f\"‚úÖ Hybrid search pipeline created successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to create pipeline: {response.text}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating pipeline: {e}\")\n",
        "        return False\n",
        "\n",
        "# Create the pipeline\n",
        "print(\"üîß Setting up hybrid search pipeline...\")\n",
        "create_hybrid_search_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipeline_explanation"
      },
      "source": [
        "**Hybrid Search Pipeline Components:**\n",
        "- **Min-Max Normalization**: Scales all scores to 0-1 range\n",
        "- **Harmonic Mean**: Combines keyword and semantic scores\n",
        "- **Weights**: 30% keyword search + 70% semantic search\n",
        "- **Result**: Balanced search that leverages both approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section5_header"
      },
      "source": [
        "## Section 5: Document Download and Preparation\n",
        "\n",
        "Download sample documents for our search system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_corpus"
      },
      "outputs": [],
      "source": [
        "def download_corpus():\n",
        "    \"\"\"Download sample markdown documents\"\"\"\n",
        "    os.makedirs('./corpus_input', exist_ok=True)\n",
        "    \n",
        "    urls = [\n",
        "        (\"https://storage.googleapis.com/llm-course/md/1.md\", \"./corpus_input/1.md\"),\n",
        "        (\"https://storage.googleapis.com/llm-course/md/2.md\", \"./corpus_input/2.md\"),\n",
        "        (\"https://storage.googleapis.com/llm-course/md/44.md\", \"./corpus_input/44.md\"),\n",
        "        (\"https://storage.googleapis.com/llm-course/md/5555.md\", \"./corpus_input/5555.md\")\n",
        "    ]\n",
        "    \n",
        "    downloaded_count = 0\n",
        "    for url, path in urls:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"üì• Downloading {url.split('/')[-1]}...\")\n",
        "            try:\n",
        "                urllib.request.urlretrieve(url, path)\n",
        "                downloaded_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Failed to download {url}: {e}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ {path.split('/')[-1]} already exists\")\n",
        "    \n",
        "    print(f\"üìö Downloaded {downloaded_count} new files\")\n",
        "    return len(urls)\n",
        "\n",
        "# Download documents\n",
        "print(\"üì• Downloading sample documents...\")\n",
        "total_files = download_corpus()\n",
        "print(f\"üìÅ Total files available: {total_files}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_explanation"
      },
      "source": [
        "**Document Preparation:**\n",
        "- Creates `./corpus_input` directory\n",
        "- Downloads 4 sample Markdown files\n",
        "- Checks for existing files to avoid re-downloading\n",
        "- These documents will be our searchable knowledge base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section6_header"
      },
      "source": [
        "## Section 6: Document Loading and Parsing\n",
        "\n",
        "Load documents and split them into searchable chunks (nodes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_parse_documents"
      },
      "outputs": [],
      "source": [
        "# Load Markdown documents from directory\n",
        "reader = SimpleDirectoryReader(\n",
        "    input_dir=\"./corpus_input\",\n",
        "    recursive=True,\n",
        "    required_exts=[\".md\", \".markdown\"]\n",
        ")\n",
        "\n",
        "print(\"üìñ Loading documents...\")\n",
        "documents = reader.load_data()\n",
        "print(f\"‚úÖ Loaded {len(documents)} documents successfully\")\n",
        "\n",
        "# Display document info\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"üìÑ Document {i+1}: {len(doc.text)} characters\")\n",
        "\n",
        "# Create parser for Markdown\n",
        "print(\"\\nüîß Creating Markdown parser...\")\n",
        "md_parser = MarkdownNodeParser()\n",
        "nodes = md_parser.get_nodes_from_documents(documents)\n",
        "print(f\"‚úÖ Created {len(nodes)} nodes with MarkdownNodeParser\")\n",
        "\n",
        "# Display sample node\n",
        "if nodes:\n",
        "    print(f\"\\nüìù Sample node preview:\")\n",
        "    print(f\"Text length: {len(nodes[0].text)} characters\")\n",
        "    print(f\"Preview: {nodes[0].text[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "parsing_explanation"
      },
      "source": [
        "**Document Processing Steps:**\n",
        "1. **SimpleDirectoryReader**: Reads all `.md` files from the directory\n",
        "2. **MarkdownNodeParser**: Splits documents into logical chunks based on Markdown structure\n",
        "3. **Nodes**: Individual searchable units that maintain context\n",
        "4. **Benefits**: Better search accuracy and relevance than whole documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section7_header"
      },
      "source": [
        "## Section 7: Embedding Model Setup\n",
        "\n",
        "Configure the BGE-M3 model for converting text to vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "embedding_setup"
      },
      "outputs": [],
      "source": [
        "# Setup embedding model\n",
        "print(\"ü§ñ Setting up BGE-M3 embedding model...\")\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\", device=device)\n",
        "print(f\"‚úÖ BGE-M3 embedding model setup successful\")\n",
        "\n",
        "# Test embedding and get dimensions\n",
        "print(\"\\nüß™ Testing embedding model...\")\n",
        "test_text = \"This is a test sentence for embedding.\"\n",
        "embeddings = embed_model.get_text_embedding(test_text)\n",
        "dim = len(embeddings)\n",
        "\n",
        "print(f\"‚úÖ Embedding test successful\")\n",
        "print(f\"üìè Embedding dimensions: {dim}\")\n",
        "print(f\"üî¢ Sample embedding values: {embeddings[:5]}...\")  # Show first 5 values\n",
        "\n",
        "# Test with different languages (BGE-M3 is multilingual)\n",
        "test_texts = [\n",
        "    \"Hello world\",\n",
        "    \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏ä‡∏≤‡∏ß‡πÇ‡∏•‡∏Å\",\n",
        "    \"Êú∫Âô®Â≠¶‰π†\"\n",
        "]\n",
        "\n",
        "print(\"\\nüåê Testing multilingual capabilities:\")\n",
        "for text in test_texts:\n",
        "    emb = embed_model.get_text_embedding(text)\n",
        "    print(f\"'{text}' ‚Üí {len(emb)} dimensions ‚úÖ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "embedding_explanation"
      },
      "source": [
        "**BGE-M3 Model Features:**\n",
        "- **Multilingual**: Supports 100+ languages including Thai\n",
        "- **1024 Dimensions**: High-quality vector representations\n",
        "- **State-of-the-art**: One of the best embedding models available\n",
        "- **Multi-functionality**: Supports dense retrieval, sparse retrieval, and multi-vector retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section8_header"
      },
      "source": [
        "## Section 8: OpenSearch Vector Store Configuration\n",
        "\n",
        "Connect to OpenSearch and configure the vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vector_store_setup"
      },
      "outputs": [],
      "source": [
        "# Setup OpensearchVectorClient\n",
        "print(\"üîó Setting up OpenSearch Vector Client...\")\n",
        "client = OpensearchVectorClient(\n",
        "    endpoint=OPENSEARCH_ENDPOINT,\n",
        "    index=OPENSEARCH_INDEX,\n",
        "    dim=dim,\n",
        "    embedding_field=EMBEDDING_FIELD,\n",
        "    text_field=TEXT_FIELD,\n",
        "    search_pipeline=\"hybrid-search-pipeline\",\n",
        ")\n",
        "print(f\"‚úÖ OpensearchVectorClient setup successful for index '{OPENSEARCH_INDEX}'\")\n",
        "\n",
        "# Create vector store\n",
        "print(\"\\nüì¶ Creating vector store...\")\n",
        "vector_store = OpensearchVectorStore(client)\n",
        "print(\"‚úÖ Vector store created successfully\")\n",
        "\n",
        "# Create storage context\n",
        "print(\"\\nüóÑÔ∏è Creating storage context...\")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "print(\"‚úÖ Storage context created successfully\")\n",
        "\n",
        "# Display configuration summary\n",
        "print(\"\\nüìã Configuration Summary:\")\n",
        "print(f\"   üîó Endpoint: {OPENSEARCH_ENDPOINT}\")\n",
        "print(f\"   üìä Index: {OPENSEARCH_INDEX}\")\n",
        "print(f\"   üìè Vector Dimensions: {dim}\")\n",
        "print(f\"   üîç Search Pipeline: hybrid-search-pipeline\")\n",
        "print(f\"   üìù Text Field: {TEXT_FIELD}\")\n",
        "print(f\"   üî¢ Embedding Field: {EMBEDDING_FIELD}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vector_store_explanation"
      },
      "source": [
        "**OpenSearch Vector Store Components:**\n",
        "- **VectorClient**: Handles communication with OpenSearch\n",
        "- **Vector Store**: LlamaIndex interface for vector operations\n",
        "- **Storage Context**: Defines where and how to store vectors\n",
        "- **Search Pipeline**: Uses our hybrid search configuration\n",
        "- **Index Schema**: Automatically created based on our specifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section9_header"
      },
      "source": [
        "## Section 9: Index Creation and Storage\n",
        "\n",
        "Create the final searchable index and save it for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_index"
      },
      "outputs": [],
      "source": [
        "# Create the index\n",
        "print(\"üèóÔ∏è Creating VectorStoreIndex...\")\n",
        "print(\"   This process will:\")\n",
        "print(\"   1. Convert all text nodes to embeddings\")\n",
        "print(\"   2. Store vectors in OpenSearch\")\n",
        "print(\"   3. Create searchable index\")\n",
        "print(\"\\n‚è≥ This may take a few minutes...\")\n",
        "\n",
        "index = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "\n",
        "print(\"‚úÖ VectorStoreIndex created successfully!\")\n",
        "\n",
        "# Save index with pickle for future use\n",
        "index_filename = f\"{OPENSEARCH_INDEX}.pkl\"\n",
        "print(f\"\\nüíæ Saving index to {index_filename}...\")\n",
        "\n",
        "try:\n",
        "    with open(index_filename, 'wb') as f:\n",
        "        pickle.dump(index, f)\n",
        "    print(f\"‚úÖ Index saved to {index_filename} successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to save index: {e}\")\n",
        "\n",
        "# Display final statistics\n",
        "print(\"\\nüìä Final Statistics:\")\n",
        "print(f\"   üìö Documents processed: {len(documents)}\")\n",
        "print(f\"   üîß Nodes created: {len(nodes)}\")\n",
        "print(f\"   üî¢ Vector dimensions: {dim}\")\n",
        "print(f\"   üì¶ Index name: {OPENSEARCH_INDEX}\")\n",
        "print(f\"   üíæ Saved as: {index_filename}\")\n",
        "\n",
        "print(\"\\nüéâ All processes completed successfully!\")\n",
        "print(\"üöÄ Your hybrid search system is now ready to use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "index_creation_explanation"
      },
      "source": [
        "**Index Creation Process:**\n",
        "1. **Embedding Generation**: Each text node is converted to a 1024-dim vector\n",
        "2. **OpenSearch Storage**: Vectors and text are stored in OpenSearch\n",
        "3. **Index Structure**: Creates searchable index with hybrid capabilities\n",
        "4. **Pickle Storage**: Saves LlamaIndex object for easy reloading\n",
        "\n",
        "**What You Now Have:**\n",
        "- ‚úÖ Fully functional hybrid search system\n",
        "- ‚úÖ OpenSearch index with your documents\n",
        "- ‚úÖ Saved index file for future use\n",
        "- ‚úÖ Ready to answer questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps_header"
      },
      "source": [
        "## Next Steps: Testing Your Search System\n",
        "\n",
        "Now you can test your hybrid search system without needing an LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_search"
      },
      "outputs": [],
      "source": [
        "# ===== üîç ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ =====\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ OpenAI API key\n",
        "Settings.llm = None\n",
        "\n",
        "# Quick test of the search system\n",
        "print(\"\\nüîç Testing the search system...\")\n",
        "\n",
        "# Create retriever instead of query_engine\n",
        "retriever = index.as_retriever(\n",
        "    vector_store_query_mode=VectorStoreQueryMode.HYBRID,\n",
        "    similarity_top_k=3\n",
        ")\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    \"What is machine learning?\",\n",
        "    \"How does artificial intelligence work?\", \n",
        "    \"Explain neural networks\",\n",
        "    \"‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á\"  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
        "]\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\nüîç Test Query {i}: {query}\")\n",
        "    try:\n",
        "        nodes = retriever.retrieve(query)\n",
        "        print(f\"‚úÖ Found {len(nodes)} relevant documents:\")\n",
        "        for j, node in enumerate(nodes, 1):\n",
        "            print(f\"  {j}. Score: {node.score:.3f}\")\n",
        "            print(f\"     Content: {node.text[:150]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "print(\"\\nüéØ Search system is working! Documents are being retrieved successfully.\")\n",
        "\n",
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö keyword vs semantic\n",
        "print(\"\\nüî¨ Testing different search modes...\")\n",
        "\n",
        "# Semantic search only\n",
        "print(\"\\nüß† Semantic Search:\")\n",
        "semantic_retriever = index.as_retriever(\n",
        "    vector_store_query_mode=VectorStoreQueryMode.DEFAULT,\n",
        "    similarity_top_k=2\n",
        ")\n",
        "\n",
        "# Keyword search (text search)\n",
        "print(\"\\nüîç Hybrid Search vs Semantic Search:\")\n",
        "# Note: OpenSearch hybrid search ‡∏à‡∏∞‡∏£‡∏ß‡∏° keyword + semantic ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß\n",
        "\n",
        "test_query = \"machine learning algorithms\"\n",
        "semantic_results = semantic_retriever.retrieve(test_query)\n",
        "hybrid_results = retriever.retrieve(test_query)\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"Semantic only: {len(semantic_results)} results\")\n",
        "print(f\"Hybrid search: {len(hybrid_results)} results\")\n",
        "\n",
        "print(\"\\n‚ú® Hybrid search test completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "üéâ **Congratulations!** You have successfully built a hybrid search system that combines:\n",
        "\n",
        "### Key Achievements:\n",
        "- ‚úÖ **Semantic Search**: Understanding context and meaning\n",
        "- ‚úÖ **Keyword Search**: Exact term matching\n",
        "- ‚úÖ **Hybrid Results**: Best of both worlds\n",
        "- ‚úÖ **Multilingual Support**: Works with multiple languages\n",
        "- ‚úÖ **Scalable Architecture**: Ready for production use\n",
        "\n",
        "### System Components:\n",
        "1. **BGE-M3 Embeddings**: State-of-the-art text representations\n",
        "2. **OpenSearch**: Powerful search and analytics engine\n",
        "3. **LlamaIndex**: Seamless document management\n",
        "4. **Hybrid Pipeline**: Intelligent result combination\n",
        "\n",
        "### Future Enhancements:\n",
        "- Add more documents to expand the knowledge base\n",
        "- Implement question-answering capabilities\n",
        "- Create a web interface for easier interaction\n",
        "- Add document filtering and metadata search\n",
        "- Implement user feedback and result ranking\n",
        "\n",
        "Your hybrid search system is now ready to handle complex queries and provide accurate, contextual results! üöÄ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
